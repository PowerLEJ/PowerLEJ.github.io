---
layout: single
title:  "AI Class 01 01 21 통계학 기본"
categories: python
tag: python
toc: true
author_profile: false
sidebar:
    nav: "docs"
search: true
---

![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https://{{ site.url | remove_first: 'https://' | remove_first: 'http://' }}{{ page.url }}&count_bg=%23FFCF00&title_bg=%230045FF&icon=macys.svg&icon_color=%23FAFF00&title=hits&edge_flat=false)

## 모수  
- 통계적 모델링은 적절한 가정 위에서 확률분포를 추정(inference)하는 것이 목표이며, 기계학습과 통계학이 공통적으로 추구하는 목표  
- 그러나 유한한 개수의 데이터만 관찰해서 모집단의 분포를 정확하게 알아내는 것은 불가능하므로, 근사적으로 확률분포를 추정할 수밖에 없다  
- 예측모형의 목적은 분포를 정확하게 맞추는 것보다는 데이터와 추정 방법의 불확실성을 고려해서 위험을 최소화하는 것  
- 데이터가 특정 확률분포를 따른다고 선험적으로(a priori) 가정한 후 그 분포를 결정하는 모수(parameter)를 추정하는 방법은 모수적(parametric) 방법론이라 한다  
- 특정 확률분포를 가정하지 않고 데이터에 따라 모델의 구조 및 모수의 개수가 유연하게 바뀌면 비모수(nonparametric) 방법론이라 부른다  

### 확률분포 가정하기 : 예제  
- 확률분포를 가정하는 방법 : 우선 히스토그램을 통해 모양을 관찰한다  
    - 데이터가 2개의 값(0 또는 1)만 가지는 경우 -> 베르누이분포  
    - 데이터가 n개의 이산적인 값을 가지는 경우 -> 카테고리분포  
    - 데이터가 [0,1] 사이에서 값을 가지는 경우 -> 베타분포  
    - 데이터가 0 이상의 값을 가지는 경우 -> 감마분포, 로그정규분포 등  
    - 데이터가 R 전체에서 값을 가지는 경우 -> 정규분포, 라플라스분포 등  
- 기계적으로 확률분포를 가정해서는 안되며, 데이터를 생성하는 원리를 먼저 고려하는 것이 원칙  

### 데이터로 모수를 추정해보자  
- 데이터의 확률분포를 가정했다면 모수를 추정해볼 수 있다  
- 정규분포의 모수는 평균![AI_class_01_01_21_01](/images/2022-02-05-AI_class_01_01_21/AI_class_01_01_21_01.png)과 분산![AI_class_01_01_21_02](/images/2022-02-05-AI_class_01_01_21/AI_class_01_01_21_02.png)으로 이를 추정하는 통계량(statistic)은 다음과 같다  

![AI_class_01_01_21_03](/images/2022-02-05-AI_class_01_01_21/AI_class_01_01_21_03.png){: width="50%" height="50%"}{: .center}  

- 통계량의 확률분포를 표집분표(sampling distribution)라 부르며, 특히 표본평균의 표집분포는 N이 커질수록 정규분포 ![AI_class_01_01_21_04](/images/2022-02-05-AI_class_01_01_21/AI_class_01_01_21_04.png)를 따릅니다  
- 이를 중심극한정리(Central Limit Theorem)이라 부르며, 모집단의 분포가 정규분포를 따르지 않아도 성립한다  

## 최대가능도 추정법  
- 표본평균이나 표본분산은 중요한 통계량이지만 확률분포마다 사용하는 모수가 다르므로 적절한 통계량이 달라지게 된다  
- 이론적으로 가장 가능성이 높은 모수를 추정하는 방법 중 하나는 최대가능도 추정법(maximum likelihood estimation, MLE)  

![AI_class_01_01_21_05](/images/2022-02-05-AI_class_01_01_21/AI_class_01_01_21_05.png){: width="50%" height="50%"}{: .center}  

- 데이터 집합 X가 독립적으로 추출되었을 경우 로그가능도를 최적화한다  

![AI_class_01_01_21_06](/images/2022-02-05-AI_class_01_01_21/AI_class_01_01_21_06.png){: width="50%" height="50%"}{: .center}  

### 왜 로그가능도를 사용하나요?  
- 로그가능도를 최적화하는 모수 ![AI_class_01_01_21_07](/images/2022-02-05-AI_class_01_01_21/AI_class_01_01_21_07.png)는 가능도를 최적화하는 MLE가 된다  
- 데이터의 숫자가 적으면 상관없지만 만일 데이터의 숫자가 수억단위가 된다면 컴퓨터의 정확도로는 가능도를 계산하는 것이 불가능  
- 데이터가 독립일 경우 로그를 사용하면 가능도의 곱셈을 로그가능도의 덧셈으로 바꿀 수 있기 때문에 컴퓨터로 연산이 가능해진다  
- 경사하강법으로 가능도를 최적화할 때 미분 연산을 사용하게 되는데 로그가능도를 사용하면 연산량을 O(n²)에서 O(n)으로 줄여준다  
- 대게의 손실함수의 경우 경사하강법을 사용하므로 음의 로그가능도(negative log-likelihood)를 최적화하게 된다  

### 최대가능도 추정법 에제 : 정규분포  
- 정규분포를 따르는 확률변수 X로부터 독립적인 표본 ![AI_class_01_01_21_08](/images/2022-02-05-AI_class_01_01_21/AI_class_01_01_21_08.png)을 얻었을 때 최대가능도 추정법을 이용하여 모수를 추정하면?  


![AI_class_01_01_21_09](/images/2022-02-05-AI_class_01_01_21/AI_class_01_01_21_09.png){: width="50%" height="50%"}{: .center}  

![AI_class_01_01_21_10](/images/2022-02-05-AI_class_01_01_21/AI_class_01_01_21_10.png){: width="50%" height="50%"}{: .center}  

![AI_class_01_01_21_11](/images/2022-02-05-AI_class_01_01_21/AI_class_01_01_21_11.png){: width="50%" height="50%"}{: .center}  


### 최대가능도 추정법 예제 : 카테고리 분포  
- 카테고리 분포 ![AI_class_01_01_21_12](/images/2022-02-05-AI_class_01_01_21/AI_class_01_01_21_12.png)를 따르는 확률변수 X로부터 독립적인 표본 ![AI_class_01_01_21_08](/images/2022-02-05-AI_class_01_01_21/AI_class_01_01_21_08.png)을 얻었을 때 최대가능도 추정법을 이용하여 모수를 추정한다면?  

![AI_class_01_01_21_13](/images/2022-02-05-AI_class_01_01_21/AI_class_01_01_21_13.png){: width="50%" height="50%"}{: .center}  

![AI_class_01_01_21_14](/images/2022-02-05-AI_class_01_01_21/AI_class_01_01_21_14.png){: width="50%" height="50%"}{: .center}  

![AI_class_01_01_21_15](/images/2022-02-05-AI_class_01_01_21/AI_class_01_01_21_15.png){: width="50%" height="50%"}{: .center}  

![AI_class_01_01_21_16](/images/2022-02-05-AI_class_01_01_21/AI_class_01_01_21_16.png){: width="50%" height="50%"}{: .center}  

![AI_class_01_01_21_17](/images/2022-02-05-AI_class_01_01_21/AI_class_01_01_21_17.png){: width="50%" height="50%"}{: .center}  


### 딥러닝에서 최대가능도 추정법  
- 최대가능도 추정법을 이용해서 기계학습 모델을 학습할 수 있다  
- 딥러닝 모델의 가중치를 ![AI_class_01_01_21_18](/images/2022-02-05-AI_class_01_01_21/AI_class_01_01_21_18.png)라 표기했을 때 분류 문제에서 소프트맥스 벡터는 카테고리분포의 모수 ![AI_class_01_01_21_19](/images/2022-02-05-AI_class_01_01_21/AI_class_01_01_21_19.png)를 모델링한다  
- 원핫벡터로 표현한 정답레이블 ![AI_class_01_01_21_20](/images/2022-02-05-AI_class_01_01_21/AI_class_01_01_21_20.png)을 관찰데이터로 이용해 확률분포인 소프트맥스 벡터의 로그가능도를 최적화할 수 있다  

![AI_class_01_01_21_21](/images/2022-02-05-AI_class_01_01_21/AI_class_01_01_21_21.png){: width="50%" height="50%"}{: .center}  

### 확률분포의 거리를 구해보자  
- 기계학습에서 사용되는 손실함수들은 모델이 학습하는 확률분포와 데이터에서 관찰되는 확률분포의 거리를 통해 유도한다  
- 데이터공간에 두 개의 확률분포 P(x), Q(x)가 있을 경우 두 확률분포 사이의 거리(distance)를 계산할 때 다음과 같은 함수들을 이용한다  
    - 총변동 거리(Total Variation Distance, TV)  
    - 쿨백-라이블러 발산(Kullback-Leibler Divergence, KL)  
    - 바슈타인 거리(Wasserstein Distance)  

### 쿨백 - 라이블러 발산  
- 쿨백-라이블러 발산(KL Divergence)은 다음과 같이 정의한다  

![AI_class_01_01_21_22](/images/2022-02-05-AI_class_01_01_21/AI_class_01_01_21_22.png){: width="50%" height="50%"}{: .center}  

- 쿨백 라이블러는 다음과 같이 분해할 수 있다  

![AI_class_01_01_21_23](/images/2022-02-05-AI_class_01_01_21/AI_class_01_01_21_23.png){: width="50%" height="50%"}{: .center}  

- 분류 문제에서 정답레이블을 P, 모델 예측을 Q라 두면 최대가능도 추정법은 쿨백-라이블러 발산을 최소화하는 것과 같다  














임성빈 교수의 인공지능(AI) 강의를 들으며 정리하였습니다.  
또한 이곳저곳에서 구글링하며 공부하였습니다.  