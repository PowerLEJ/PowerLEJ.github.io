---
layout: single
title:  "AI Class 01 01 15 경사하강법 2"
categories: python
tag: python
toc: true
author_profile: false
sidebar:
    nav: "docs"
search: true
---

![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https://{{ site.url | remove_first: 'https://' | remove_first: 'http://' }}{{ page.url }}&count_bg=%23FFCF00&title_bg=%230045FF&icon=macys.svg&icon_color=%23FAFF00&title=hits&edge_flat=false)

## 경사하강법으로 선형회귀 계수 구하기  
- 선형회귀의 목적식은 ![AI_class_01_01_15_01](/images/2022-01-30-AI_class_01_01_15/AI_class_01_01_15_01.png){: .center}이고 이를 최소화하는 ![AI_class_01_01_15_02](/images/2022-01-30-AI_class_01_01_15/AI_class_01_01_15_02.png){: .center}를 찾아야 하므로 다음과 같은 그레디언트 벡터를 구해야 한다  

![AI_class_01_01_15_03](/images/2022-01-30-AI_class_01_01_15/AI_class_01_01_15_03.png){: width="50%" height="50%"}{: .center}  

![AI_class_01_01_15_04](/images/2022-01-30-AI_class_01_01_15/AI_class_01_01_15_04.png){: width="50%" height="50%"}{: .center}  

![AI_class_01_01_15_05](/images/2022-01-30-AI_class_01_01_15/AI_class_01_01_15_05.png){: width="50%" height="50%"}{: .center}  

- 목적식을 최소화 하는 ![AI_class_01_01_15_02](/images/2022-01-30-AI_class_01_01_15/AI_class_01_01_15_02.png){: .center}를 구하는 경사하강법 알고리즘  

![AI_class_01_01_15_06](/images/2022-01-30-AI_class_01_01_15/AI_class_01_01_15_06.png){: width="50%" height="50%"}{: .center}  

![AI_class_01_01_15_07](/images/2022-01-30-AI_class_01_01_15/AI_class_01_01_15_07.png){: width="50%" height="50%"}{: .center}  

- ![AI_class_01_01_15_01](/images/2022-01-30-AI_class_01_01_15/AI_class_01_01_15_01.png){: .center} 대신 ![AI_class_01_01_15_10](/images/2022-01-30-AI_class_01_01_15/AI_class_01_01_15_10.png){: .center}를 최소화하면 식이 좀 더 간단해진다  

![AI_class_01_01_15_08](/images/2022-01-30-AI_class_01_01_15/AI_class_01_01_15_08.png){: width="50%" height="50%"}{: .center}  

![AI_class_01_01_15_09](/images/2022-01-30-AI_class_01_01_15/AI_class_01_01_15_09.png){: width="50%" height="50%"}{: .center}  

## 경사하강법 기반 선형회귀 알고리즘  
- Input : X, y, lr, T / Output : beta  
- norm : L2-노름을 계산하는 함수  
- lr : 학습률  
- T : 학습 횟수  

```python
for t in range(T): # 종료조건을 일정 학습횟수로 변경한 점만 빼고 앞에서 배운 경사하강법 알고리즘과 같다
    error = y - X @ beta
    grad = transpose(X) @ error
    beta = beta - ls * grad
```  

![AI_class_01_01_15_10](/images/2022-01-30-AI_class_01_01_15/AI_class_01_01_15_10.png){: .center} 항을 계산해서 ![AI_class_01_01_15_02](/images/2022-01-30-AI_class_01_01_15/AI_class_01_01_15_02.png){: .center}를 업데이트 한다  

```python
import numpy as np

X = np.array([[1,1], [1,2], [2,2], [2,3]])
print(f"X : {X}")
y = np.dot(X, np.array([1,2])) + 3
print(f"y : {y}")

beta_gd = [10.1, 15.1, -6.5] # [1,2,3]이 정답
print(f"beta_gd : {beta_gd}")
X_ = np.array([np.append(x,[1]) for x in X]) # intercept 항 추가
print(f"X_ : {X_}")

for t in range(5000):
    error = y - X_ @ beta_gd
    print("---------------------------")
    print(f"error : {error}")
    # error = error / np.linalg.norm(error)
    grad = -np.transpose(X_) @ error
    print(f"grad : {grad}")
    beta_gd = beta_gd - 0.01 * grad
    print(f"beta_gd : {beta_gd}")
    print("---------------------------")
    
print(f"beta_gd : {beta_gd}")
```  
>  
결과  
```
    X : [[1 1]
    [1 2]
    [2 2]
    [2 3]]
    y : [ 6  8  9 11]
    beta_gd : [10.1, 15.1, -6.5]
    X_ : [[1 1 1]
    [1 2 1]
    [2 2 1]
    [2 3 1]]
    ---------------------------
    error : [-12.7 -25.8 -34.9 -48. ]
    grad : [204.3 278.1 121.4]
    beta_gd : [ 8.057 12.319 -7.714]
    ---------------------------
    ---------------------------
    error : [ -6.662 -16.981 -24.038 -34.357]
    grad : [140.433 191.771  82.038]
    beta_gd : [ 6.65267 10.40129 -8.53438]
    ---------------------------
    ---------------------------
    error : [ -2.51958 -10.92087 -16.57354 -24.97483]
    grad : [ 96.53719 132.43289  54.98882]
    beta_gd : [ 5.6872981  9.0769611 -9.0842682]
    ---------------------------
    ~~~~~~~~~~~~~~~~~~~~ 생 략 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    ---------------------------
    error : [ 1.69064343e-06  2.20389320e-06 -1.49797573e-06 -9.84725959e-07]
    grad : [ 1.07086675e-06 -1.48300497e-07 -1.41183494e-06]
    beta_gd : [1.00000369 1.99999949 2.99999513]
    ---------------------------
    ---------------------------
    error : [ 1.68575074e-06  2.19751751e-06 -1.49364276e-06 -9.81875989e-07]
    grad : [ 1.06776923e-06 -1.47872289e-07 -1.40774951e-06]
    beta_gd : [1.00000368 1.99999949 2.99999515]
    ---------------------------
    ---------------------------
    error : [ 1.68087222e-06  2.19116026e-06 -1.48932231e-06 -9.79034267e-07]
    grad : [ 1.06468068e-06 -1.47445320e-07 -1.40367590e-06]
    beta_gd : [1.00000367 1.99999949 2.99999516]
    ---------------------------
    beta_gd : [1.00000367 1.99999949 2.99999516]
```  

- 경사하강법 알고리즘에서는 학습률과 학습횟수가 중요한 hyperparameter가 된다  

```python
import numpy as np

X = np.array([[1,1], [1,2], [2,2], [2,3]])
y = np.dot(X, np.array([1,2])) + 3

beta_gd = [10.1, 15.1, -6.5] # [1,2,3]이 정답
X_ = np.array([np.append(x,[1]) for x in X]) # intercept 항 추가

for t in range(100):
    error = y - X_ @ beta_gd
    # error = error / np.linalg.norm(error)
    grad = -np.transpose(X_) @ error
    beta_gd = beta_gd - 0.01 * grad
    
print(f"beta_gd : {beta_gd}")
```  
>  
결과  
```
    beta_gd : [ 3.41314549  4.63604548 -6.69249764]
```  

### 경사하강법은 만능일까?  
- 이론적으로 경사하강법은 미분가능하고 볼록(convex)한 함수에 대해서는 적절한 학습률과 학습횟수를 선택했을 때 수렴이 보장되어 있다  
- 볼록한 함수는 그레디언트 벡터가 항상 최소점을 향한다  
- 특히 선형회귀의 경우 목적식 ![AI_class_01_01_15_01](/images/2022-01-30-AI_class_01_01_15/AI_class_01_01_15_01.png){: .center}은 회귀계수 ![AI_class_01_01_15_02](/images/2022-01-30-AI_class_01_01_15/AI_class_01_01_15_02.png){: .center}에 대해 볼록함수이기 때문에 알고리즘을 충분히 돌리면 수렴이 보장된다  

![AI_class_01_01_15_11](/images/2022-01-30-AI_class_01_01_15/AI_class_01_01_15_11.png){: width="50%" height="50%"}{: .center}  

- 하지만 비선형회귀 문제의 경우 목적식이 볼록하지 않을 수 있으므로 수렴이 항상 보장되지는 않는다  
- 특히 딥러닝을 사용하는 경우 목적식은 대부분 볼록함수가 아니다  

![AI_class_01_01_15_12](/images/2022-01-30-AI_class_01_01_15/AI_class_01_01_15_12.png){: width="50%" height="50%"}{: .center}  

## 확률적 경사하강법  
- 확률적 경사하강법(stochastic gradient descent)은 모든 데이터를 사용해서 업데이트 하는 대신 데이터 한개 또는 일부 활용하여 업데이트 한다  
- 볼록이 아닌(non-convex) 목적식은 SGD를 통해 최적화할 수 있다  
- SGD라고 해서 만능은 아니지만 딥러닝의 경우 SGD가 경사하강법보다 실증적으로 더 낫다고 검증되었다  

![AI_class_01_01_15_13](/images/2022-01-30-AI_class_01_01_15/AI_class_01_01_15_13.png){: width="50%" height="50%"}{: .center}  

- SGD는 데이터의 일부를 가지고 패러미터를 업데이트 하기 때문에 연산자원을 좀 더 효율적으로 활용하는 데 도움이 된다  

![AI_class_01_01_15_14](/images/2022-01-30-AI_class_01_01_15/AI_class_01_01_15_14.png){: width="50%" height="50%"}{: .center}  

### 확률적 경사하강법의 원리 : 미니배치 연산  
- 경사하강법은 전체데이터 ![AI_class_01_01_15_15](/images/2022-01-30-AI_class_01_01_15/AI_class_01_01_15_15.png){: .center}를 가지고 목적식의 그레디언트 벡터인 ![AI_class_01_01_15_16](/images/2022-01-30-AI_class_01_01_15/AI_class_01_01_15_16.png){: .center}를 계산한다  

![AI_class_01_01_15_17](/images/2022-01-30-AI_class_01_01_15/AI_class_01_01_15_17.png){: width="50%" height="50%"}{: .center}  

- SGD는 미니배치 ![AI_class_01_01_15_18](/images/2022-01-30-AI_class_01_01_15/AI_class_01_01_15_18.png){: .center}를 가지고 그레디언트 벡터를 계산한다. 미니배치는 확률적으로 선택하므로 목적식 모양이 바뀌게 된다  

![AI_class_01_01_15_19](/images/2022-01-30-AI_class_01_01_15/AI_class_01_01_15_19.png){: width="50%" height="50%"}{: .center}  

![AI_class_01_01_15_20](/images/2022-01-30-AI_class_01_01_15/AI_class_01_01_15_20.png){: width="50%" height="50%"}{: .center}  

- SGD는 볼록이 아닌 목적식에서도 사용 가능하므로 경사하강법보다 머신러닝 학습에 더 효율적이다  

![AI_class_01_01_15_21](/images/2022-01-30-AI_class_01_01_15/AI_class_01_01_15_21.png){: width="50%" height="50%"}{: .center}  


임성빈 교수의 인공지능(AI) 강의를 들으며 정리하였습니다.  
또한 이곳저곳에서 구글링하며 공부하였습니다.  