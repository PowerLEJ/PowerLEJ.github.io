---
layout: single
title:  "AI Class 01 01 10 Python data handling"
categories: python
tag: python
toc: true
author_profile: false
sidebar:
    nav: "docs"
search: true
---

![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https://{{ site.url | remove_first: 'https://' | remove_first: 'http://' }}{{ page.url }}&count_bg=%23FFCF00&title_bg=%230045FF&icon=macys.svg&icon_color=%23FAFF00&title=hits&edge_flat=false)


## Comma Separate Values CSV  
- CSV, 필드를 쉼표(,)로 구분한 텍스트 파일  
- 엑셀 양식의 데이터를 프로그램에 상관 없이 쓰기 위한 데이터 형식  
- 탭(TSV), 빈칸(SSV) 등으로 구분해서 만들기도 함  
- 통칭하여 Comma Separate Values(CSV) 라고 부름  
- 엑셀에서는 "다른 이름 저장" 기능으로 사용 가능  

-- Python 실습을 위해 customer.csv 다운로드(https://bit.ly/3psoUZb)  

```python
line_counter = 0 # 파일의 총 줄수를 세는 변수
data_header = [] # data의 필드값을 저장하는 list
customer_list = [] # customer 개별 List를 저장하는 List

with open("customers.csv") as customer_data: # customer.csv파일을 customer_data객체에 저장
   
    while True:
        data = customer_data.readline() # customer.csv에 한줄씩 data변수에 저장
        if not data: break # 데이터가 없을 때 Loop 종료
        if line_counter == 0: # 첫번째 데이터는 데이터 필드
            data_header = data.split(",") # 일반데이터는 customer_list객체에 저장, 데이터 저장 시 ","로 분리
        else:
            customer_list.append(data.split(",")) # 일반데이터는 customer_list객체에 저장, 데이터 저장 시 ","로 분리
        line_counter += 1

print("Header : \t", data_header) # 데이터필드값 출력
for i in range(0, 10): # 데이터 출력 (샘플 10개만)
    print("Data", i, ":\t\t", customer_list[i])
print(len(customer_list)) # 전체데이터 크기 출력
```  
>  
결과  
```
Header :         ['customerNumber', 'customerName', 'contactLastName', 'contactFirstName', 'phone', 'addressLine1', 'addressLine2', 'city', 'state', 'postalCode', 'country', 'salesRepEmployeeNumber', 'creditLimit\n']
Data 0 :                 ['103', '"Atelier graphique"', 'Schmitt', '"Carine "', '40.32.2555', '"54', ' rue Royale"', 'NULL', 'Nantes', 'NULL', '44000', 'France', '1370', '21000\n']
Data 1 :                 ['112', '"Signal Gift Stores"', 'King', 'Jean', '7025551838', '"8489 Strong St."', 'NULL', '"Las Vegas"', 'NV', '83030', 'USA', '1166', '71800\n']
Data 2 :                 ['114', '"Australian Collectors', ' Co."', 'Ferguson', 'Peter', '"03 9520 4555"', '"636 St Kilda Road"', '"Level 3"', 'Melbourne', 'Victoria', '3004', 'Australia', '1611', '117300\n']
Data 3 :                 ['119', '"La Rochelle Gifts"', 'Labrune', '"Janine "', '40.67.8555', '"67', ' rue des Cinquante Otages"', 'NULL', 'Nantes', 'NULL', '44000', 'France', '1370', '118200\n']
Data 4 :                 ['121', '"Baane Mini Imports"', 'Bergulfsen', '"Jonas "', '"07-98 9555"', '"Erling Skakkes gate 78"', 'NULL', 'Stavern', 'NULL', '4110', 'Norway', '1504', '81700\n']
Data 5 :                 ['124', '"Mini Gifts Distributors Ltd."', 'Nelson', 'Susan', '4155551450', '"5677 Strong St."', 'NULL', '"San Rafael"', 'CA', '97562', 'USA', '1165', '210500\n']
Data 6 :                 ['125', '"Havel & Zbyszek Co"', 'Piestrzeniewicz', '"Zbyszek "', '"(26) 642-7555"', '"ul. Filtrowa 68"', 'NULL', 'Warszawa', 'NULL', '01-012', 'Poland', 'NULL', '0\n']
Data 7 :                 ['128', '"Blauer See Auto', ' Co."', 'Keitel', 'Roland', '"+49 69 66 90 2555"', '"Lyonerstr. 34"', 'NULL', 'Frankfurt', 'NULL', '60528', 'Germany', '1504', '59700\n']
Data 8 :                 ['129', '"Mini Wheels Co."', 'Murphy', 'Julie', '6505555787', '"5557 North Pendale Street"', 'NULL', '"San Francisco"', 'CA', '94217', 'USA', '1165', '64600\n']
Data 9 :                 ['131', '"Land of Toys Inc."', 'Lee', 'Kwai', '2125557818', '"897 Long Airport Avenue"', 'NULL', 'NYC', 'NY', '10022', 'USA', '1323', '114900\n']
122
```  

```python
line_counter = 0
data_header = []
employee = []
customer_USA_only_list = []
customer = None

with open("customers.csv", "r") as customer_data:
    while 1:
        data = customer_data.readline()
        if not data:
            break
        if line_counter == 0:
            data_header = data.split(",")
        else:
            customer = data.split(",")
            if customer[10].upper() == "USA": # customer 데이터의 offset 10번째 값
                customer_USA_only_list.append(customer) # 즉 country 필드가 "USA"인 것만
        line_counter += 1 # customer_USA_only_list에 저장

print("Header : \t", data_header)
for i in range(0, 10):
    print("Data : \t\t", customer_USA_only_list[i])
print(len(customer_USA_only_list))

with open("customers_USA_only.csv", "w") as customer_USA_only_csv:
    for customer in customer_USA_only_list:
        customer_USA_only_csv.write(",".join(customer).strip('\n')+"\n") # customer_USA_only_list객체에 있는 데이터를 customers_USA_only.csv파일에 쓰기
```  
>  
결과  
```
    Header :         ['customerNumber', 'customerName', 'contactLastName', 'contactFirstName', 'phone', 'addressLine1', 'addressLine2', 'city', 'state', 'postalCode', 'country', 'salesRepEmployeeNumber', 'creditLimit\n']
    Data :           ['112', '"Signal Gift Stores"', 'King', 'Jean', '7025551838', '"8489 Strong St."', 'NULL', '"Las Vegas"', 'NV', '83030', 'USA', '1166', '71800\n']
    Data :           ['124', '"Mini Gifts Distributors Ltd."', 'Nelson', 'Susan', '4155551450', '"5677 Strong St."', 'NULL', '"San Rafael"', 'CA', '97562', 'USA', '1165', '210500\n']
    Data :           ['129', '"Mini Wheels Co."', 'Murphy', 'Julie', '6505555787', '"5557 North Pendale Street"', 'NULL', '"San Francisco"', 'CA', '94217', 'USA', '1165', '64600\n']
    Data :           ['131', '"Land of Toys Inc."', 'Lee', 'Kwai', '2125557818', '"897 Long Airport Avenue"', 'NULL', 'NYC', 'NY', '10022', 'USA', '1323', '114900\n']
    Data :           ['151', '"Muscle Machine Inc"', 'Young', 'Jeff', '2125557413', '"4092 Furth Circle"', '"Suite 400"', 'NYC', 'NY', '10022', 'USA', '1286', '138500\n']
    Data :           ['157', '"Diecast Classics Inc."', 'Leong', 'Kelvin', '2155551555', '"7586 Pompton St."', 'NULL', 'Allentown', 'PA', '70267', 'USA', '1216', '100600\n']
    Data :           ['161', '"Technics Stores Inc."', 'Hashimoto', 'Juri', '6505556809', '"9408 Furth Circle"', 'NULL', 'Burlingame', 'CA', '94217', 'USA', '1165', '84600\n']
    Data :           ['168', '"American Souvenirs Inc"', 'Franco', 'Keith', '2035557845', '"149 Spinnaker Dr."', '"Suite 101"', '"New Haven"', 'CT', '97823', 'USA', '1286', '0\n']
    Data :           ['173', '"Cambridge Collectables Co."', 'Tseng', 'Jerry', '6175555555', '"4658 Baden Av."', 'NULL', 'Cambridge', 'MA', '51247', 'USA', '1188', '43400\n']
    Data :           ['175', '"Gift Depot Inc."', 'King', 'Julie', '2035552570', '"25593 South Bay Ln."', 'NULL', 'Bridgewater', 'CT', '97562', 'USA', '1323', '84300\n']
    34
```  

### CSV객체로 CSV 처리  
- Text파일 형태로 데이터 처리 시 문장 내에 들어가 있는 "," 등에 대해 전처리 과정이 필요  
- 파이썬에서는 간단히 CSV파일을 처리하기 위해 csv객체를 제공함  
- 예제데이터: korea_foot_traffic_data.csv (from http://www.data.go.kr)  
- 예제데이터는 국내 주요 상권의 유동인구 현황 정보  
- 한글로 되어 있어 한글 처리가 필요  
- 시간대/조사일자/행정구역/날씨 등을 기준으로 연령별/성별 유동인구 해당 지역에 몇 명인가 표시  

#### CSV 객체 활용  

```python
    import csv
    reader = csv.reader(f,
            delimiter=',', quotechar='"',
            quoting=csv.QUOTE_ALL)
```  

- delimiter ( , ) : 글자를 나누는 기준  
- lineterminator ( \r\n ) : 줄바꿈 기준  
- quotechar ( " ) : 문자열을 둘러싸는 신호 문자  
- quoting ( QUOTE_MONIMAL ) : 데이터 나누는 기준이 quotechar에 의해 둘러싸인 레벨  

```python
import csv
seoung_nam_data = []
header = []
rownum = 0

with open("korea_floating_population_data.csv","r", encoding="cp949") as p_file:
    csv_data = csv.reader(p_file) #csv 객체를 이용해서 csv_data 읽기
    for row in csv_data: #읽어온 데이터를 한 줄씩 처리
        if rownum == 0:
            header = row #첫 번째 줄은 데이터 필드로 따로 저장
        location = row[7]
        #“행정구역”필드 데이터 추출, 한글 처리로 유니코드 데이터를 cp949로 변환
        if location.find(u"성남시") != -1: # u는 유니코드의 약자
            seoung_nam_data.append(row)
        #”행정구역” 데이터에 성남시가 들어가 있으면 seoung_nam_data List에 추가
        rownum +=1

with open("seoung_nam_floating_population_data.csv", "w", encoding="utf8") as s_p_file:
    writer = csv.writer(s_p_file, delimiter='\t', quotechar="'", quoting=csv.QUOTE_ALL)
    # csv.writer를 사용해서 csv 파일 만들기 delimiter 필드 구분자
    # quotechar는 필드 각 데이터는 묶는 문자, quoting는 묶는 범위
    writer.writerow(header) #제목 필드 파일에 쓰기
    for row in seoung_nam_data:
        writer.writerow(row) #seoung_nam_data에 있는 정보 list에 쓰기
```  

## Web  
- World Wide Web (WWW), 줄여서 웹  
- 우리가 늘 쓰는 인터넷 공간의 정식 명칭  
- 팀 버너스리에 의해 1989년 처음 제안되었으며, 원래는 물리학자들간 정보 교환을 위해 사용됨  
- 데이터 송수신을 위한 HTTP프로토콜 사용, 데이터를 표시하기 위해 HTML형식을 사용  

### HTML (Hyper Text Markup Language)  
- 웹 상의 정보를 구조적으로 표현하기 위한 언어  
- 제목, 단락, 링크 등 요소 표시를 위해 Tag 사용  
- 모든 요소들은 꺾쇠 괄호 안에 둘러 쌓여 있음  
- 모든 HTML은 트리 모양의 포함관계를 가짐  
- 일반적으로 웹페이지의 HTML소스파일은 컴퓨터가 다운로드 받은 후 웹브라우저가 해석/표시  

### 정규식 regular expression  
- 정규표현식, regexp 또는 regex 등으로 불림  
- 복잡한 문자열 패턴을 정의하는 문자 표현 공식  
- 특정한 규칙을 가진 문자열의 집합을 추출  

-- 정규식 for HTML Parsing  
- 주민등록번호, 전화번호, 도서 ISBN 등 형식이 있는 문자열을 원본 문자열로부터 추출함  
- HTML 역시 tag를 사용한 일정한 형식이 존재하여 정규식으로 추출이 용이함  
- 관련자료: http://www.nextree.co.kr/p4327  
- 문법 자체는 매우 방대, 스스로 찾아서 공부 필요  
- 필요한 것들은 인터넷 검색  
- 기본적인 것을 공부한 후 넓게 적용하는 것 중요  

-- 정규식 연습장 활용하기  
1. 정규식 연습장 이동 http://www.regexr.com  
2. 테스트하고 싶은 문서를 text란에 삽입  
3. 정규식을 사용해서 찾아보기  

### 정규식 기본 문법 1  
- 문자클래스 [] : [ 와 ] 사이의 문자들과 매치라는 의미  
    예) [abc] <- 해당 글자가 a,b,c 중 하나가 있다  
    "a", "before", "deep", "dud", "sunset"  

- "-" : 사용 범위를 지정할 수 있음  
    예) [a-zA-Z] : 알파벳 전체, [0-9] : 숫자 전체  

- 정규식 표현을 위해 원래 의미 X, 다른 용도로 사용되는 문자 : . ^ $ * + ? { } [ ] \ &#124; ( )  

- . : 줄바꿈 문자인 \ n을 제외한 모든 문자와 매치 a[.]b  
- &#42; 별표임 : 앞에 있는 글자를 반복해서 나올 수 있음 tomor*ow tomorrow tomoow tomorrrrow  
- &#43; 플러스임 : 앞에 있는 글자를 최소 1회 이상 반복  
- {m.n} : 반복 횟수를 지정 {1,} , {0,} , {1,3}  
    203.252.101.40 [0-9]{1,3} \ d {1,3}  
- ? : 반복 횟수가 1회 01[01]?-[0-9]{4}-[0-9]{4}  
- &#124; 역슬래시임 : or (0&#124;1){3}  
- ^ : not  

#### 정규식 추출 연습  
1. 정규식 연습장 http://www.regexr.com 으로 이동  
2. 구글 USPTO Bulk Download 데이터페이지 소스 보기 클릭  
3. 소스 전체 복사 후 정규식 연습장 페이지에 붙여 넣기  
4. 상단 Expression부분을 수정해가며 "Zip"로 끝나는 파일명만 추출  
5. Expression에 (http)(.+)(zip)를 입력 # http로 시작해서 zip으로 끝나는데 그 사이는 무슨 값이든 상관없다  


### 파이썬에서 정규식 사용  
- re 모듈을 import 하여 사용 : import re  
- 함수 search : 한 개만 찾기  
- 함수 findall : 전체 찾기  
- 추출된 패턴은 tuple로 반환됨  
- 연습 : 특정 페이지에서 ID만 추출하기 https://bit.ly/3rxQFS4  
- ID 패턴: [영문대소문자|숫자] 여러개, 별표로 끝남  
"([A-Za-z0-9])" 정규식

```python
import re
import urllib.request

url = "https://bit.ly/3rxQFS4"
html = urllib.request.urlopen(url)
html_contents = str(html.read())
print(html_contents)
id_results = re.findall(r"([A-Za-z0-9]+\*\*\*)", html_contents) # findall 전체 찾기, 패턴대로 데이터 찾기

for result in id_results:
    print(result)
```  

```python
import urllib.request # urllib 모듈 호출
import re
url = "http://www.google.com/googlebooks/uspto-patents-grants-text.html"
#url 값 입력
html = urllib.request.urlopen(url) # url 열기
html_contents = str(html.read().decode("utf8"))
# html 파일 읽고, 문자열로 변환
url_list = re.findall(r"(http)(.+)(zip)", html_contents)
for url in url_list:
    print("".join(url)) # 출력된 Tuple 형태 데이터 str으로 join
```  
>  
결과  
```
    http://storage.googleapis.com/patents/grant_full_text/1995/pftaps19950704_wk27.zip
    http://storage.googleapis.com/patents/grant_full_text/1995/pftaps19950711_wk28.zip
    http://storage.googleapis.com/patents/grant_full_text/1995/pftaps19950718_wk29.zip
    http://storage.googleapis.com/patents/grant_full_text/1995/pftaps19950725_wk30.zip
    http://storage.googleapis.com/patents/grant_full_text/1995/pftaps19950801_wk31.zip
    http://storage.googleapis.com/patents/grant_full_text/1995/pftaps19950808_wk32.zip
    http://storage.googleapis.com/patents/grant_full_text/1995/pftaps19950815_wk33.zip
    ~~~~~~~~~~~~~~~~~~생략~~~~~~~~~~~~~~~
    http://storage.googleapis.com/patents/grant_full_text/1976/pftaps19761109_wk45.zip
    http://storage.googleapis.com/patents/grant_full_text/1976/pftaps19761116_wk46.zip
    http://storage.googleapis.com/patents/grant_full_text/1976/pftaps19761123_wk47.zip
    http://storage.googleapis.com/patents/grant_full_text/1976/pftaps19761130_wk48.zip
    http://storage.googleapis.com/patents/grant_full_text/1976/pftaps19761207_wk49.zip
    http://storage.googleapis.com/patents/grant_full_text/1976/pftaps19761214_wk50.zip
    http://storage.googleapis.com/patents/grant_full_text/1976/pftaps19761221_wk51.zip
    http://storage.googleapis.com/patents/grant_full_text/1976/pftaps19761228_wk52.zip
```  

```python
import urllib.request
import re
url = "http://finance.naver.com/item/main.nhn?code=005930"
html = urllib.request.urlopen(url)
html_contents = str(html.read().decode("ms949"))
stock_results = re.findall("(\<dl class=\"blind\"\>)([\s\S]+?)(\<\/dl\>)", html_contents)
samsung_stock = stock_results[0] # 두 개 tuple 값중 첫번째 패턴
samsung_index = samsung_stock[1] # 세 개의 tuple 값중 두 번째 값
# 하나의 괄호가 tuple index가 됨
index_list= re.findall("(\<dd\>)([\s\S]+?)(\<\/dd\>)", samsung_index)
for index in index_list:
    print (index[1]) # 세 개의 tuple 값중 두 번째 값
```  

### eXtensible Markup Language  
#### XML 이란  
- 데이터의 구조와 의미를 설명하는 TAG(MarkUp)를 사용하여 표시하는 언어  
- TAG와 TAG 사이에 값이 표시되고, 구조적인 정보를 표현할 수 있음  
- HTML과 문법이 비슷, 대표적인 데이터 저장 방식  
- 정보의 구조에 대한 정보인 스키마와 DTD 등으로 정보에 대한 정보(메타정보)가 표현되며, 용도에 따라 다양한 형태로 변경 가능  
- XML은 컴퓨터(예: PC <-> 스마트폰) 간에 정보를 주고 받기 매우 유용한 저장 방식으로 쓰이고 있음  

### 파이썬에서 XML Parsing  
- XML도 HTML과 같은 구조적 markup 언어  
- 정규표현식으로 Parging이 가능함  
- 그러나 좀 더 손쉬운 도구들이 개발되어 있음  
- 가장 많이 쓰이는 parser인 beautifulsoup으로 파싱  

#### BeautifulSoup  
- HTML, XML 등 Markup 언어 Scraping을 위한 대표적인 도구  
- https://www.crummy.com/software/BeautifulSoup  
- lxml과 html5lib와 같은 Parser를 사용함  
- 속도는 상대적으로 느리나 간편히 사용할 수 있음  

-- beautifulsoup 설치  
- conda 가상 환경으로 lxml과 beautifulsoup 설치  

```python
    conda install lxml
    conda install -c anaconda beautifullsoup4
```  

-- beautifulsoup 모듈 사용  

```python
    # 모듈 호출
    from bs4 import BeautifulSoup
    # 객체 생성
    soup = BeautifulSoup(Books_xml, "lxml")
    # Tag 찾는 함수 find_all 생성
    soup.find_all("author")
```  

- find_all : 정규식과 마찬가지로 해당 패턴을 모두 반환  
- find('invention-title')  
    Tag 네임 = title  
- get_text() : 반환된 패턴의 값 반환 (태그와 태그 사이)  

-- beutifulsoup 예제  
- 데이터 다운로드 받기  
- https://s3.ap-northeast-2.amazonaws.com/teamlab-gachon/books.xml  

```python
<?xml version="1.0"?>
  <books>
    <book>
        <author>Carson</author>
        <price format="dollar">31.95</price>
        <pubdate>05/01/2001</pubdate>
    </book>
    <pubinfo>
        <publisher>MSPress</publisher>
        <state>WA</state>
    </pubinfo>
    <book>
        <author>Sungchul</author>
        <price format="dollar">29.95</price>
        <pubdate>05/01/2012</pubdate>
    </book>
    <pubinfo>
        <publisher>Gachon</publisher>
        <state>SeoungNam</state>
    </pubinfo>
  </books>
```  

```python
from bs4 import BeautifulSoup

with open("books.xml", "r", encoding="utf8") as books_file:
    books_xml = books_file.read() # File을 String으로 읽어오기

# xml 모듈을 사용해서 데이터 분석
soup = BeautifulSoup(books_xml, "lxml")
# author가 들어간 모든 element 추출
for book_info in soup.find_all("author"):
    print(book_info)
    print(book_info.get_text())
```  
>  
결과  
```
    <author>Carson</author>
    Carson
    <author>Sungchul</author>
    Sungchul
```  

## JSON  
- JavaScript Object Notation  
- 원래 웹 언어인 Java Script의 데이터 객체 표현 방식  
- 간결성으로 기계/인간이 모두 이해하기 편함  
- 데이터 용량이 적고, Code로의 전환이 쉬움  
- 이로 인해 XML의 대체제로 많이 활용되고 있음  

### 파이썬에서 JSON  
- json 모듈을 사용하여 손 쉽게 파싱 및 저장 가능  
- 데이터 저장 및 읽기는 dict type과 상호 호환 가능  
- 웹에서 제공하는 API는 대부분 정보 교환 시 JSON 활용  
- 페북, 트위터, 깃헙 등 거의 모든 사이트  
- 각 사이트마다 Developer API의 활용법을 찾아 사용  

#### JSON Read  
- JSON 파일 구조 확인 -> 읽어온 후 -> Dict Type처럼 처리  

- JSON Data  

```json
    {"employees":[
        {"firstName":"John", "lastName":"Doe"},
        {"firstName":"Anna", "lastName":"Smith"},
        {"firstName":"Peter", "lastName":"Jones"}
    ]}    
```  

```python
    import json

    with open("json_example.json", "r", encoding="utf8") as f:
        contents = f.read()
        json_data = json.loads(contents)
        print(json_data["employees"])

    print(type(json_data))

    for employee in json_data["employees"]:
        print(employee["lastName"])
```  
>  
결과  
```
    [{'firstName': 'John', 'lastName': 'Doe'}, {'firstName': 'Anna', 'lastName': 'Smith'}, {'firstName': 'Peter', 'lastName': 'Jones'}]
    <class 'dict'>
    Doe
    Smith
    Jones
```  

#### JSON Write  
- Dict Type으로 데이터 저장 -> json모듈로 write  

```python
    import json

    dict_data = {'Name': 'Zara', 'Age': 7, 'Class': 'First'}

    with open("data.json", "w") as f:
        json.dump(dict_data, f)
```  





































최성철 교수의 인공지능(AI) 강의를 들으며 정리하였습니다.  
또한 이곳저곳에서 구글링하며 공부하였습니다.  