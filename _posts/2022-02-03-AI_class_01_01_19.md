---
layout: single
title:  "AI Class 01 01 19 확률론 기본"
categories: python
tag: python
toc: true
author_profile: false
sidebar:
    nav: "docs"
search: true
---

![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https://{{ site.url | remove_first: 'https://' | remove_first: 'http://' }}{{ page.url }}&count_bg=%23FFCF00&title_bg=%230045FF&icon=macys.svg&icon_color=%23FAFF00&title=hits&edge_flat=false)

## 확률론  

### 딥러닝에서 확률론이 필요한 이유  
- 딥러닝은 확률론 기반의 기계학습 이론에 바탕을 두고 있다  
- 기계학습에서 사용되는 손실함수(loss function)들의 작동 원리는 데이터 공간을 통계적으로 해석해서 유도하게 된다  
- 예측이 틀릴 위험(risk)을 최소화하도록 데이터를 학습하는 원리는 통계적 기계학습의 기본 원리이다  
- 회귀 분석에서 손실함수로 사용되는 L2-노름은 예측 오차의 분산을 가장 최소화하는 방향으로 학습하도록 유도한다  
- 분류 문제에서 사용되는 교차엔트로피(cross-entropy)는 모델 예측의 불확실성을 최소화하는 방향으로 학습하도록 유도한다  
- 분산 및 불확실성을 최소화하기 위해서는 측정하는 방법을 알아야 한다  

### 확률분포는 데이터의 초상화  
- 데이터공간을 X * Y 라 표기하고 D는 데이터공간에서 데이터를 추출하는 분포  
- 데이터는 확률변수로 (x, y) ~ D라 표기  
- 결합분포 P(x, y)는 D를 모델링한다  
- P(x)는 입력 x에 대한 주변확률분포이고, y에 대한 정보를 주진 않는다  

![AI_class_01_01_19_04](/images/2022-02-03-AI_class_01_01_19/AI_class_01_01_19_04.png){: width="50%" height="50%"}{: .center}  

![AI_class_01_01_19_05](/images/2022-02-03-AI_class_01_01_19/AI_class_01_01_19_05.png){: width="50%" height="50%"}{: .center}  

- 조건부확률분포 P(x&#124;y)는 데이터공간에서 입력 x와 출력 y 사이의 관계를 모델링한다  

![AI_class_01_01_19_06](/images/2022-02-03-AI_class_01_01_19/AI_class_01_01_19_06.png){: width="50%" height="50%"}{: .center}  

![AI_class_01_01_19_01](/images/2022-02-03-AI_class_01_01_19/AI_class_01_01_19_01.png){: width="50%" height="50%"}{: .center}  

### 이산확률변수 vs 연속확률변수  
- 확률변수는 확률분포 D에 따라 이산형(discrete)과 연속형(continuous) 확률변수로 구분하게 된다 (X * Y 에 의해 결정되는 것이 아닌)  
- 이산형 확률변수는 확률변수가 가질 수 있는 경우의 수를 모두 고려하여 확률을 더해서 모델링 한다  

![AI_class_01_01_19_02](/images/2022-02-03-AI_class_01_01_19/AI_class_01_01_19_02.png){: width="50%" height="50%"}{: .center}  

- 연속형 확률변수는 데이터 공간에 정의된 확률변수의 밀도(density) 위에서의 적분을 통해 모델링 한다  

![AI_class_01_01_19_03](/images/2022-02-03-AI_class_01_01_19/AI_class_01_01_19_03.png){: width="50%" height="50%"}{: .center}  

### 조건부확률과 기계학습  
- 조건부확률 P(y&#124;x)는 입력변수 x에 대해 정답이 y일 확률을 의마한다  
- 연속확률분포의 경우 P(y&#124;x)는 확률이 아니고 밀도로 해석한다는 것을 주의하자  
- 로지스틱 회귀에서 사용했던 선형모델과 소프트맥스 함수의 결합은 데이터에서 추출된 패턴을 기반으로 확률을 해석하는 데 사용된다  
- 분류 문제에서 softmax(W ![AI_class_01_01_19_07](/images/2022-02-03-AI_class_01_01_19/AI_class_01_01_19_07.png)   + b)는 데이터 x로부터 추출된 특징패턴 ![AI_class_01_01_19_07](/images/2022-02-03-AI_class_01_01_19/AI_class_01_01_19_07.png)(x)와 가중치행렬 W를 통해 조건부확률 P(y&#124;x)를 계산한다  

![AI_class_01_01_19_08](/images/2022-02-03-AI_class_01_01_19/AI_class_01_01_19_08.png){: width="50%" height="50%"}{: .center}  

- 회귀문제의 경우 조건부기대값 E[y&#124;x]을 추정한다  

![AI_class_01_01_19_09](/images/2022-02-03-AI_class_01_01_19/AI_class_01_01_19_09.png){: width="50%" height="50%"}{: .center}  

- 딥러닝은 다층신경망을 사용하여 데이터로부터 특징패턴 ![AI_class_01_01_19_07](/images/2022-02-03-AI_class_01_01_19/AI_class_01_01_19_07.png)을 추출한다  

### 기대값  
- 확률분포가 주어지면 데이터를 분석하는 데 사용 가능한 여러 종류의 통계적 범함수(statistical functional)를 게산할 수 있다  
- 기대값(expectation)은 데이터를 대표하는 통계량이면서 동시에 확률분포를 통해 다른 통계적 범함수를 계산하는 데 사용된다  

![AI_class_01_01_19_10](/images/2022-02-03-AI_class_01_01_19/AI_class_01_01_19_10.png){: width="50%" height="50%"}{: .center}  

- 기대값을 이용해 분산, 첨도, 공분산 등 여러 통계량을 계산할 수 있다  

![AI_class_01_01_19_11](/images/2022-02-03-AI_class_01_01_19/AI_class_01_01_19_11.png){: width="50%" height="50%"}{: .center}  

## 몬테카를로 샘플링  
- 기계학습의 많은 문제들은 확률분포를 명시적으로 모를 때가 대부분이다  
- 확률분포를 모를 때 데이터를 이용하여 기대값을 계산하려면 몬테카를로(Monte Carlo) 샘플링 방법을 사용해야 한다  

![AI_class_01_01_19_12](/images/2022-02-03-AI_class_01_01_19/AI_class_01_01_19_12.png){: width="50%" height="50%"}{: .center}  

- 몬테카를로 샘플링은 독립추출만 보장된다면 대수의 법칙(law of large number)에 의해 수렴성을 보장한다  


### 몬테카를로 예제 : 적분 계산하기  

![AI_class_01_01_19_13](/images/2022-02-03-AI_class_01_01_19/AI_class_01_01_19_13.png){: width="50%" height="50%"}{: .center}  


```python
import numpy as np

def mc_int(fun, low, high, sample_size=100, repeat=10):
    int_len = np.abs(high - low)
    stat = []
    for _ in range(repeat):
        x = np.random.uniform(low=low, high=high, size=sample_size)
        fun_x = fun(x)
        int_val = int_len * np.mean(fun_x)
        stat.append(int_val)
    return np.mean(stat), np.std(stat)

def f_x(x):
    return np.exp(-x**2)

print(mc_int(f_x, low=-1, high=1, sample_size=10000, repeat=100))
```  
>  
result  
```
    (1.493961360863905, 0.004439854162744381)
```  

임성빈 교수의 인공지능(AI) 강의를 들으며 정리하였습니다.  
또한 이곳저곳에서 구글링하며 공부하였습니다.  