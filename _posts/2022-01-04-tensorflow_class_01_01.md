---
layout: single
title:  "Tensorflow Class 01 01"
categories: tensorflow
tag: tensorflow
toc: true
author_profile: false
sidebar:
    nav: "docs"
search: true
---

![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https://{{ site.url | remove_first: 'https://' | remove_first: 'http://' }}{{ page.url }}&count_bg=%23FFCF00&title_bg=%230045FF&icon=macys.svg&icon_color=%23FAFF00&title=hits&edge_flat=false)

## Study
```python
import tensorflow as tf
msg = tf.constant('hello world')
tf.print(msg)
```
>
결과
```
    hello world
```    

### MNIST 4분할 데이터 불러오기

X_train에는 총 60000개의 28*28 크기의 이미지가 담겨 있다  
Y_train에는 X_train의 60000개에 대한 값(0~9)이 담겨 있는 레이블 데이터셋이다  
X_train과 Y_train으로 모델을 학습한 후에 X_test, Y_test를 이용해서 학습된 모델의 정확도를 평가한다  

```python
# MNIST 손글씨 데이터 package 수입
mnist = tf.keras.datasets.mnist

# MNIST 4분할 데이터 불러오기
(X_train, Y_train), (X_test, Y_test) = mnist.load_data()
print('학습용 입력 데이터 모양:', X_train.shape)
print('학습용 출력 데이터 모양:', Y_train.shape)
print('평가용 입력 데이터 모양:', X_test.shape)
print('평가용 출력 데이터 모양:', Y_test.shape)
```
>
결과
```
    학습용 입력 데이터 모양: (60000, 28, 28)
    학습용 출력 데이터 모양: (60000,)
    평가용 입력 데이터 모양: (10000, 28, 28)
    평가용 출력 데이터 모양: (10000,)
```    

### 이미지 데이터 원본 출력
```python
# 이미지 데이터 원본 출력
import matplotlib.pyplot as plt
plt.imshow(X_train[0], cmap='gray')
plt.show()

print('첫 번째 학습용 데이터 입력값:', X_train[0])
print('첫 번째 학습용 데이터 출력값:', Y_train[0])
```

![tensor_class_01_01](/images/2022-01-04-tensorflow_class_01_01/tensor_class_01_01_01.png){: width="100%" height="100%"}{: .center}

>
결과
```
    첫 번째 학습용 데이터 입력값: [[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
        0   0   0   0   0   0   0   0   0   0]
    [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
        0   0   0   0   0   0   0   0   0   0]
    [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
        0   0   0   0   0   0   0   0   0   0]
    [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
        0   0   0   0   0   0   0   0   0   0]
    [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
        0   0   0   0   0   0   0   0   0   0]
    [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136
    175  26 166 255 247 127   0   0   0   0]
    [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253
    225 172 253 242 195  64   0   0   0   0]
    [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251
    93  82  82  56  39   0   0   0   0   0]
    [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241
        0   0   0   0   0   0   0   0   0   0]
    [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154
        0   0   0   0   0   0   0   0   0   0]
    [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0
        0   0   0   0   0   0   0   0   0   0]
    [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0
        0   0   0   0   0   0   0   0   0   0]
    [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0
        0   0   0   0   0   0   0   0   0   0]
    [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1
        0   0   0   0   0   0   0   0   0   0]
    [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119
    25   0   0   0   0   0   0   0   0   0]
    [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253
    150  27   0   0   0   0   0   0   0   0]
    [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252
    253 187   0   0   0   0   0   0   0   0]
    [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249
    253 249  64   0   0   0   0   0   0   0]
    [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253
    253 207   2   0   0   0   0   0   0   0]
    [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253
    250 182   0   0   0   0   0   0   0   0]
    [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201
    78   0   0   0   0   0   0   0   0   0]
    [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2
        0   0   0   0   0   0   0   0   0   0]
    [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0
        0   0   0   0   0   0   0   0   0   0]
    [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0
        0   0   0   0   0   0   0   0   0   0]
    [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0
        0   0   0   0   0   0   0   0   0   0]
    [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
        0   0   0   0   0   0   0   0   0   0]
    [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
        0   0   0   0   0   0   0   0   0   0]
    [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
        0   0   0   0   0   0   0   0   0   0]]
    첫 번째 학습용 데이터 출력값: 5
```    

### 이미지 데이터 [0, 1] 스케일링
```python
# 이미지 데이터 [0, 1] 스케일링
X_train = X_train / 255.0
X_test = X_test / 255.0

# 스케일링 후 데이터 확인
plt.imshow(X_train[0], cmap='gray')
plt.show()
print('첫 번째 학습용 데이터 입력값:', X_train[0])
```

![tensor_class_01_02](/images/2022-01-04-tensorflow_class_01_01/tensor_class_01_01_02.png){: width="100%" height="100%"}{: .center}

>
결과
```
    첫 번째 학습용 데이터 입력값: [[0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.        ]
    [0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.        ]
    [0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.        ]
    [0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.        ]
    [0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.        ]
    [0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.         0.         0.
    0.01176471 0.07058824 0.07058824 0.07058824 0.49411765 0.53333333
    0.68627451 0.10196078 0.65098039 1.         0.96862745 0.49803922
    0.         0.         0.         0.        ]
    [0.         0.         0.         0.         0.         0.
    0.         0.         0.11764706 0.14117647 0.36862745 0.60392157
    0.66666667 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686
    0.88235294 0.6745098  0.99215686 0.94901961 0.76470588 0.25098039
    0.         0.         0.         0.        ]
    [0.         0.         0.         0.         0.         0.
    0.         0.19215686 0.93333333 0.99215686 0.99215686 0.99215686
    0.99215686 0.99215686 0.99215686 0.99215686 0.99215686 0.98431373
    0.36470588 0.32156863 0.32156863 0.21960784 0.15294118 0.
    0.         0.         0.         0.        ]
    [0.         0.         0.         0.         0.         0.
    0.         0.07058824 0.85882353 0.99215686 0.99215686 0.99215686
    0.99215686 0.99215686 0.77647059 0.71372549 0.96862745 0.94509804
    0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.        ]
    [0.         0.         0.         0.         0.         0.
    0.         0.         0.31372549 0.61176471 0.41960784 0.99215686
    0.99215686 0.80392157 0.04313725 0.         0.16862745 0.60392157
    0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.        ]
    [0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.05490196 0.00392157 0.60392157
    0.99215686 0.35294118 0.         0.         0.         0.
    0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.        ]
    [0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.         0.         0.54509804
    0.99215686 0.74509804 0.00784314 0.         0.         0.
    0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.        ]
    [0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.         0.         0.04313725
    0.74509804 0.99215686 0.2745098  0.         0.         0.
    0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.        ]
    [0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.         0.         0.
    0.1372549  0.94509804 0.88235294 0.62745098 0.42352941 0.00392157
    0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.        ]
    [0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.         0.         0.
    0.         0.31764706 0.94117647 0.99215686 0.99215686 0.46666667
    0.09803922 0.         0.         0.         0.         0.
    0.         0.         0.         0.        ]
    [0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.         0.         0.
    0.         0.         0.17647059 0.72941176 0.99215686 0.99215686
    0.58823529 0.10588235 0.         0.         0.         0.
    0.         0.         0.         0.        ]
    [0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.0627451  0.36470588 0.98823529
    0.99215686 0.73333333 0.         0.         0.         0.
    0.         0.         0.         0.        ]
    [0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.         0.         0.97647059
    0.99215686 0.97647059 0.25098039 0.         0.         0.
    0.         0.         0.         0.        ]
    [0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.         0.         0.
    0.         0.         0.18039216 0.50980392 0.71764706 0.99215686
    0.99215686 0.81176471 0.00784314 0.         0.         0.
    0.         0.         0.         0.        ]
    [0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.         0.         0.
    0.15294118 0.58039216 0.89803922 0.99215686 0.99215686 0.99215686
    0.98039216 0.71372549 0.         0.         0.         0.
    0.         0.         0.         0.        ]
    [0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.         0.09411765 0.44705882
    0.86666667 0.99215686 0.99215686 0.99215686 0.99215686 0.78823529
    0.30588235 0.         0.         0.         0.         0.
    0.         0.         0.         0.        ]
    [0.         0.         0.         0.         0.         0.
    0.         0.         0.09019608 0.25882353 0.83529412 0.99215686
    0.99215686 0.99215686 0.99215686 0.77647059 0.31764706 0.00784314
    0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.        ]
    [0.         0.         0.         0.         0.         0.
    0.07058824 0.67058824 0.85882353 0.99215686 0.99215686 0.99215686
    0.99215686 0.76470588 0.31372549 0.03529412 0.         0.
    0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.        ]
    [0.         0.         0.         0.         0.21568627 0.6745098
    0.88627451 0.99215686 0.99215686 0.99215686 0.99215686 0.95686275
    0.52156863 0.04313725 0.         0.         0.         0.
    0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.        ]
    [0.         0.         0.         0.         0.53333333 0.99215686
    0.99215686 0.99215686 0.83137255 0.52941176 0.51764706 0.0627451
    0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.        ]
    [0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.        ]
    [0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.        ]
    [0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.         0.         0.
    0.         0.         0.         0.        ]]
```    

### 인공신경망 구현

Flatten은 입력 이미지 크기 28*28이고, 1차원 텐서이다  
Dense는 입력 받은 784(28*28)개를 128개로 인코딩 해주는데 활성함수 ReLU로 활성화 한다  
Dropout은 뉴런 128개 중 랜덤으로 20%를 무시한다  
Dense는 입력 이미지가 0~9까지의 어떤 숫자를 의미하는지 확률을 얻으려고 10개를 출력하는데 레이어의 결과값을 다중분류를 위한 확률값으로 해석할 수 있는 활성함수 Softmax로 활성화한다  

```python
# 인공신경망 구현
model = tf.keras.models.Sequential()
layers = tf.keras.layers  

model.add(layers.Flatten(input_shape=(28, 28))) # 입력 이미지 -> 은닉층1 (뉴런 784개)
model.add(layers.Dense(128, activation='relu')) # 은닉층2 (뉴런 128개 / ReLU 활성화 / dropout)
model.add(layers.Dropout(0.2))
model.add(layers.Dense(10, activation='softmax')) # 출력층 (뉴런 10개 / Softmax 활성화)

# 인공신경망 요약
model.summary()
```

>  
결과  
```  
    Model: "sequential"
    _________________________________________________________________
    Layer (type)                Output Shape              Param #   
    =================================================================
    flatten (Flatten)           (None, 784)               0         
    dense (Dense)               (None, 128)               100480    
    dropout (Dropout)           (None, 128)               0         
    dense_1 (Dense)             (None, 10)                1290      
    =================================================================
    Total params: 101,770
    Trainable params: 101,770
    Non-trainable params: 0
    _________________________________________________________________  
```    

### 인공신경망 학습

```python
# 인공신경망 학습 환경 설정
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# 인공신경망 학습
model.fit(X_train, Y_train, epochs=5, verbose=1) # verbose=0 이면 단계 콘솔 보여주지 않도록
```

Adam은 기울기 방향에 대한 경사하강법  
sparse_categorical_crossentropy는 다중 분류 손실함수  
모델 평가를 위한 평가 지표로 accuracy 지정  

Fit는 학습에 사용되는 데이터넷과 학습 반복수는 epochs=5로 지정  

>
결과
```
    Epoch 1/5
    1875/1875 [==============================] - 9s 5ms/step - loss: 0.2911 - accuracy: 0.9166
    Epoch 2/5
    1875/1875 [==============================] - 9s 5ms/step - loss: 0.1363 - accuracy: 0.9600
    Epoch 3/5
    1875/1875 [==============================] - 9s 5ms/step - loss: 0.1029 - accuracy: 0.9696
    Epoch 4/5
    1875/1875 [==============================] - 9s 5ms/step - loss: 0.0817 - accuracy: 0.9750
    Epoch 5/5
    1875/1875 [==============================] - 10s 5ms/step - loss: 0.0695 - accuracy: 0.9782
```   

### 인공신경망 평가

evaluate는 평가 데이터셋에 대한 손실값과 정확도가 표시된다  

```python
# 인공신경망 평가
model.evaluate(X_test, Y_test, verbose=1)
```

>
결과
```
    313/313 [==============================] - 1s 4ms/step - loss: 0.0738 - accuracy: 0.9760
```   

### 인공신경망 예측
```python
# 인공신경망 예측
pick = X_test[0].reshape(1, 28, 28)
pred = model.predict(pick)
answer = tf.argmax(pred, axis=1)

print('\n인공신경망 추측 결과 (원본):', pred)
print('인공신경망 추측 결과 (해석):', answer)
print('정답:', Y_test[0])
```

>
결과
```
    인공신경망 추측 결과 (원본): [[1.7354901e-07 1.7669045e-08 1.0861105e-06 8.4940525e-04 3.9864272e-11
    5.9423172e-07 8.5641190e-12 9.9906927e-01 1.4264450e-06 7.8045319e-05]]
    인공신경망 추측 결과 (해석): tf.Tensor([7], shape=(1,), dtype=int64)
    정답: 7
```   

임성규 교수의 TensorFlow로 배우는 머신러닝 알고리즘 강의를 들으며 정리하였습니다.  
또한 이곳저곳에서 구글링하며 공부하였습니다.  
