---
layout: single
title:  "AI Class 01 01 24 Linear regression"
categories: python
tag: python
toc: true
author_profile: false
sidebar:
    nav: "docs"
search: true
---

![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https://{{ site.url | remove_first: 'https://' | remove_first: 'http://' }}{{ page.url }}&count_bg=%23FFCF00&title_bg=%230045FF&icon=macys.svg&icon_color=%23FAFF00&title=hits&edge_flat=false)


### Data deffinition  

1Hours(x) 공부해서 2Points(y)  
2Hours(x) 공부해서 4Points(y)  
3Hours(x) 공부해서 6Points(y)  
4Hours(x) 공부해서 ?Points(y)  

데이터는 torch.tensor  
입력 : x_train  
출력 : y_train  

### Hypothesis  

y = Wx + b  

- Weight와 Bias 0으로 초기화 (항상 출력 0을 예측)  

    - W : weight  
    - b : Bias  

- requires_grad=True (학습할 것이라고 명시)  

### Compute loss  

![AI_class_01_01_24_01](/images/2022-02-08-AI_class_01_01_24/AI_class_01_01_24_01.png){: width="50%" height="50%"}{: .center}  

### Gradient descent  

- torch.optim 라이브러리 사용  
    - [W, b]는 학습할 tensor들  
    - lr=0.01 은 learning rate  

- 항상 붙어다니는 3줄  
    - zero_grad()로 gradient초기화  
    - backward()로 gradient 계산  
    - step()으로 개선  

```python
import torch
import torch.optim as optim

x_train = torch.FloatTensor([[1],[2],[3]])
y_train = torch.FloatTensor([[4],[5],[6]])

W = torch.zeros(1, requires_grad=True)
b = torch.zeros(1, requires_grad=True)

optimizer = optim.SGD([W, b], lr=0.01)

nb_epochs = 1000
for epoch in range(1, nb_epochs + 1):
    hypothesis = x_train * W + b
    cost = torch.mean((hypothesis - y_train) ** 2) # torch.mean으로 평균 계산

    optimizer.zero_grad()
    cost.backward()
    optimizer.step()
```  

이승재 교수의 인공지능(AI) 강의를 들으며 정리하였습니다.  
또한 이곳저곳에서 구글링하며 공부하였습니다.  