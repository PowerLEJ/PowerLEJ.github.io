---
layout: single
title:  "AI Class 01 01 17 딥러닝 학습방법 이해하기"
categories: python
tag: python
toc: true
author_profile: false
sidebar:
    nav: "docs"
search: true
---

![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https://{{ site.url | remove_first: 'https://' | remove_first: 'http://' }}{{ page.url }}&count_bg=%23FFCF00&title_bg=%230045FF&icon=macys.svg&icon_color=%23FAFF00&title=hits&edge_flat=false)

## 비선형모델인 신경망(neural network)  

![AI_class_01_01_17_01](/images/2022-02-01-AI_class_01_01_17/AI_class_01_01_17_01.png){: width="50%" height="50%"}{: .center}  

![AI_class_01_01_17_02](/images/2022-02-01-AI_class_01_01_17/AI_class_01_01_17_02.png){: width="50%" height="50%"}{: .center}  

![AI_class_01_01_17_03](/images/2022-02-01-AI_class_01_01_17/AI_class_01_01_17_03.png){: width="50%" height="50%"}{: .center}  

### 소프트맥스 연산  
- 소프트맥스(softmax)함수는 모델의 출력을 확률로 해석할 수 있게 변환해주는 연산  
- 분류 문제를 풀 때 선형모델과 소프트맥스 함수를 결합하여 예측  

![AI_class_01_01_17_04](/images/2022-02-01-AI_class_01_01_17/AI_class_01_01_17_04.png){: width="50%" height="50%"}{: .center}  

![AI_class_01_01_17_05](/images/2022-02-01-AI_class_01_01_17/AI_class_01_01_17_05.png){: width="50%" height="50%"}{: .center}  

```python
def softmax(vec):
    denumerator = np.exp(vec - np.max(vec, axis=-1, keepdims=True))
    numerator = np.sum(denumerator, axis=-1, keepdims=True)
    val = denumerator / numerator
    return val

vac = np.array([[1,2,0],[-1,0,1],[-10,0,10]])
softmax(vac)
```  
>  
result  
```
    array([[2.44728471e-01, 6.65240956e-01, 9.00305732e-02],
        [9.00305732e-02, 2.44728471e-01, 6.65240956e-01],
        [2.06106005e-09, 4.53978686e-05, 9.99954600e-01]])
```  

- 그러나 추론을 할 때는 원-핫(one-hot)벡터로 최대값을 가진 주소만 1로 출력하는 연산을 사용해서 softmax를 사용하진 않는다  

```python
def one_hot(val, dim):
    return [np.eye(dim)[_] for _ in val]

def one_hot_encoding(vec):
    vec_dim = vec.shape[1]
    vec_argmax = np.argmax(vec, axis=-1)
    return one_hot(vec_argmax, vec_dim)

def softmax(vec):
    denumerator = np.exp(vec - np.max(vec, axis=-1, keepdims=True))
    numerator = np.sum(denumerator, axis=-1, keepdims=True)
    val = denumerator / numerator
    return val

vec = np.array([[1,2,0],[-1,0,1],[-10,0,10]])
print(one_hot_encoding(vec))
print(one_hot_encoding(softmax(vec)))
```  
>  
result  
```
    [array([0., 1., 0.]), array([0., 0., 1.]), array([0., 0., 1.])]
    [array([0., 1., 0.]), array([0., 0., 1.]), array([0., 0., 1.])]
```  

#### 신경망은 선형모델과 활성함수(activation function)를 합성한 함수이다  

![AI_class_01_01_17_06](/images/2022-02-01-AI_class_01_01_17/AI_class_01_01_17_06.png){: width="50%" height="50%"}{: .center}  

### 활성함수  
- 활성함수(activation function)은 R위에 정의된 비선형(nonlinear)함수로서 딥러닝에서 매우 중요한 개념  
- 활성함수르 쓰지 않으면 딥러닝은 선형모형과 차이가 없다  
- 시그모이드(sigmoid) 함수나 tanh함수는 전통적으로 많이 쓰이던 활성함수지만 딥러닝에선 ReLU함수를 많이 쓰고 있다  

![AI_class_01_01_17_07](/images/2022-02-01-AI_class_01_01_17/AI_class_01_01_17_07.png){: width="50%" height="50%"}{: .center}  

#### 선형모델과 활성함수를 반복적으로 사용하는 것이 오늘날 사용되는 가장 기본적인 모형  

- 2층(2-layers) 신경망  

![AI_class_01_01_17_08](/images/2022-02-01-AI_class_01_01_17/AI_class_01_01_17_08.png){: width="50%" height="50%"}{: .center}  

#### 다층(multi-layer) 퍼셉트론(MLP)은 신경망이 여러층 합성된 함수  

![AI_class_01_01_17_09](/images/2022-02-01-AI_class_01_01_17/AI_class_01_01_17_09.png){: width="50%" height="50%"}{: .center}  

![AI_class_01_01_17_10](/images/2022-02-01-AI_class_01_01_17/AI_class_01_01_17_10.png){: width="50%" height="50%"}{: .center}  

![AI_class_01_01_17_11](/images/2022-02-01-AI_class_01_01_17/AI_class_01_01_17_11.png){: width="50%" height="50%"}{: .center}  


### 왜 층을 여러개 쌓아야 하는가?  
- 이론적으로는 2층 신경망으로도 임의의 연속함수를 근사할 수 있다(universal approximation theorem)  
- 그러나 층이 깊을수록 목적함수를 근사하는 데 필요한 뉴런(노드)의 숫자가 훨씬 빨리 줄어들어 좀 더 효율적으로 학습이 가능  
- 층이 얇으면 필요한 뉴런의 숫자가 기하급수적으로 늘어나서 넓은(wide) 신경망이 되어야 한다  

## 역전파 알고리즘(딥러닝 학습 원리)  
- 딥러닝은 역전파(backpropagation) 알고리즘을 이용항여 각 층에 사용된 패러미터 ![AI_class_01_01_17_12](/images/2022-02-01-AI_class_01_01_17/AI_class_01_01_17_12.png){: .center}  를 학습한다  

![AI_class_01_01_17_13](/images/2022-02-01-AI_class_01_01_17/AI_class_01_01_17_13.png){: width="50%" height="50%"}{: .center}  

- 각 층 패러미터의 그레디언트 벡터는 윗층부터 역순으로 계산하게 된다  

![AI_class_01_01_17_14](/images/2022-02-01-AI_class_01_01_17/AI_class_01_01_17_14.png){: width="50%" height="50%"}{: .center}  

### 역전파 알고리즘 원리 이해하기  
- 역전파 알고리즘은 합성함수 미분법인 연쇄법칙(chain-rule) 기반 자동미분(auto-differentiation)을 사용한다  

![AI_class_01_01_17_15](/images/2022-02-01-AI_class_01_01_17/AI_class_01_01_17_15.png){: width="50%" height="50%"}{: .center}  

## 예제: 2층 신경망  

![AI_class_01_01_17_16](/images/2022-02-01-AI_class_01_01_17/AI_class_01_01_17_16.png){: width="50%" height="50%"}{: .center}  

![AI_class_01_01_17_17](/images/2022-02-01-AI_class_01_01_17/AI_class_01_01_17_17.png){: width="50%" height="50%"}{: .center}  

임성빈 교수의 인공지능(AI) 강의를 들으며 정리하였습니다.  
또한 이곳저곳에서 구글링하며 공부하였습니다.  