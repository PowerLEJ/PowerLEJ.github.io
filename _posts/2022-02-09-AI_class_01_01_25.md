---
layout: single
title:  "AI Class 01 01 25 Deeper Look at GD"
categories: python
tag: python
toc: true
author_profile: false
sidebar:
    nav: "docs"
search: true
---

![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https://{{ site.url | remove_first: 'https://' | remove_first: 'http://' }}{{ page.url }}&count_bg=%23FFCF00&title_bg=%230045FF&icon=macys.svg&icon_color=%23FAFF00&title=hits&edge_flat=false)


### Simpler Hypothesis Function  

H(x) = Wx  

### Dummy Data  
Input = Output  

1Hours(x) 공부해서 1Points(y)  
2Hours(x) 공부해서 2Points(y)  
3Hours(x) 공부해서 3Points(y)  
4Hours(x) 공부해서 ?Points(y)  

데이터는 torch.tensor  
입력 : x_train  
출력 : y_train  

### Best Model  
- H(x) = x 가 정확한 모델  
- W = 1이 가장 좋은 숫자  

### Cost function: Intuition  
- W = 1일 때 cost=0  
- 1에서 멀어질수록 높아진다  

![AI_class_01_01_25_01](/images/2022-02-09-AI_class_01_01_25/AI_class_01_01_25_01.png){: width="50%" height="50%"}{: .center}  

### Cost function: MSE(Mean Squared Error)  
![AI_class_01_01_25_02](/images/2022-02-09-AI_class_01_01_25/AI_class_01_01_25_02.png){: width="50%" height="50%"}{: .center}  

### Gradient Descent: Intuition  
- 곡선을 내려가자  
- 기울기가 클수록 더 멀리  
- Gradient를 계산하자  

![AI_class_01_01_25_03](/images/2022-02-09-AI_class_01_01_25/AI_class_01_01_25_03.png){: width="50%" height="50%"}{: .center}  

### Gradient Descent: The Math  

![AI_class_01_01_25_04](/images/2022-02-09-AI_class_01_01_25/AI_class_01_01_25_04.png){: width="50%" height="50%"}{: .center}  

![AI_class_01_01_25_05](/images/2022-02-09-AI_class_01_01_25/AI_class_01_01_25_05.png){: width="50%" height="50%"}{: .center}  

```python
import torch

# 데이터
x_train = torch.FloatTensor([[1],[2],[3]])
y_train = torch.FloatTensor([[1],[2],[3]])

# 모델 초기화
W = torch.zeros(1)

# Learning rate 설정
lr = 0.1

nb_epochs = 10 # 데이터로 학습한 횟수
for epoch in range(1, nb_epochs + 1):
    # H(x) 계산
    hypothesis = x_train * W

    # cost gradient 계산
    cost = torch.mean((hypothesis - y_train) ** 2) # torch.mean으로 평균 계산
    gradient = torch.sum((W * x_train - y_train) * x_train)

    print('Epoch {:4d}/{} W: {:.3f}, Cost: {:.6f}'.format(
        epoch, nb_epochs, W.item(), cost.item()
    ))

    # cost gradient로 H(x) 개선
    W -= lr * gradient
```  
>  
result  
```
    Epoch    1/10 W: 0.000, Cost: 4.666667
    Epoch    2/10 W: 1.400, Cost: 0.746666
    Epoch    3/10 W: 0.840, Cost: 0.119467
    Epoch    4/10 W: 1.064, Cost: 0.019115
    Epoch    5/10 W: 0.974, Cost: 0.003058
    Epoch    6/10 W: 1.010, Cost: 0.000489
    Epoch    7/10 W: 0.996, Cost: 0.000078
    Epoch    8/10 W: 1.002, Cost: 0.000013
    Epoch    9/10 W: 0.999, Cost: 0.000002
    Epoch   10/10 W: 1.000, Cost: 0.000000
```  


### Gradient Descent with torch.optim  
- torch.optim으로도 gradient descent를 할 수 있다  

```python
import torch
import torch.optim as optim

# 데이터
x_train = torch.FloatTensor([[1],[2],[3]])
y_train = torch.FloatTensor([[1],[2],[3]])

# 모델 초기화
W = torch.zeros(1, requires_grad=True)

# optimizer 설정
optimizer = optim.SGD([W], lr=0.15)

nb_epochs = 10 # 데이터로 학습한 횟수
for epoch in range(1, nb_epochs + 1):
    # H(x) 계산
    hypothesis = x_train * W

    # cost gradient 계산
    cost = torch.mean((hypothesis - y_train) ** 2) # torch.mean으로 평균 계산

    print('Epoch {:4d}/{} W: {:.3f}, Cost: {:.6f}'.format(
        epoch, nb_epochs, W.item(), cost.item()
    ))

    # cost H(x) 개선
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()
```  
>  
result  
```
    Epoch    1/10 W: 0.000, Cost: 4.666667
    Epoch    2/10 W: 1.400, Cost: 0.746667
    Epoch    3/10 W: 0.840, Cost: 0.119467
    Epoch    4/10 W: 1.064, Cost: 0.019115
    Epoch    5/10 W: 0.974, Cost: 0.003058
    Epoch    6/10 W: 1.010, Cost: 0.000489
    Epoch    7/10 W: 0.996, Cost: 0.000078
    Epoch    8/10 W: 1.002, Cost: 0.000013
    Epoch    9/10 W: 0.999, Cost: 0.000002
    Epoch   10/10 W: 1.000, Cost: 0.000000
```  


이승재 교수의 인공지능(AI) 강의를 들으며 정리하였습니다.  
또한 이곳저곳에서 구글링하며 공부하였습니다.  