---
layout: single
title:  "AI Class 01 01 14 경사하강법 1"
categories: python
tag: python
toc: true
author_profile: false
sidebar:
    nav: "docs"
search: true
---

![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https://{{ site.url | remove_first: 'https://' | remove_first: 'http://' }}{{ page.url }}&count_bg=%23FFCF00&title_bg=%230045FF&icon=macys.svg&icon_color=%23FAFF00&title=hits&edge_flat=false)

## 미분(differentiation)  
- 변수의 움직임에 따른 함수값의 변화를 측정하기 위한 도구로 최적화에서 제일 많이 사용하는 기법  

![AI_class_01_01_14_01](/images/2022-01-29-AI_class_01_01_14/AI_class_01_01_14_01.png){: width="50%" height="50%"}{: .center}  

-- sympy.diff를 이용하여 미분을 계산한다  

```python
import sympy as sym
from sympy.abc import x

sym.diff(sym.poly(x**2 + 2*x + 3))
```  
>  
결과  
```
    Poly(2𝑥+2,𝑥,𝑑𝑜𝑚𝑎𝑖𝑛=ℤ)
```  

- 미분은 함수 f의 주어진 점 (x,f(x))에서의 접선의 기울기를 구한다  
- 한 점에서 접선의 기울기를 알면 어느 방향으로 점을 움직여야 함수값이 증가하는지 / 감소하는지 알 수 있다  
- 함수값을 증가시키고 싶으면 미분값을 더하고, 감소시키고 싶으면 미분값을 뺀다  
- **미분값을 더하면 경사상승법(gradient ascent)**이라 하며 함수의 **극대값**의 위치를 구할 때 사용  
- **미분값을 빼면 경사하강법(gradient descent)**이라 하며 함수의 **극소값**의 위치를 구할 때 사용  
- 경사상승/경사하강 방법은 극값에 도달하면 움직임을 멈춘다(극값에서는 미분값이 0이므로 더이상 업데이트가 안된다. 그러므로 목적함수 최적화가 자동으로 끝난다)  

![AI_class_01_01_14_02](/images/2022-01-29-AI_class_01_01_14/AI_class_01_01_14_02.png){: width="50%" height="50%"}{: .center}  

-- h를 0으로 보내면 (x,f(x))에서 접선의 기울기로 수렴한다  

- 미분값이 음수라서 x + f'(x) < x 는 왼쪽으로 이동하여 함수값이 증가  

![AI_class_01_01_14_03](/images/2022-01-29-AI_class_01_01_14/AI_class_01_01_14_03.png){: width="50%" height="50%"}{: .center}  

- 미분값이 양수라서 x + f'(x) > x 는 오른쪽으로 이동하여 함수값이 증가  

![AI_class_01_01_14_04](/images/2022-01-29-AI_class_01_01_14/AI_class_01_01_14_04.png){: width="50%" height="50%"}{: .center}  

- 미분값이 음수라서 x - f'(x) > x 는 오른쪽으로 이동하여 함수값이 감소  

![AI_class_01_01_14_05](/images/2022-01-29-AI_class_01_01_14/AI_class_01_01_14_05.png){: width="50%" height="50%"}{: .center}  

- 미분값이 양수라서 x - f'(x) < x 는 왼쪽으로 이동하여 함수값이 감소  

![AI_class_01_01_14_06](/images/2022-01-29-AI_class_01_01_14/AI_class_01_01_14_06.png){: width="50%" height="50%"}{: .center}  

## 경사하강법 알고리즘  
- Input : gradient, init, lr, eps / Output : var  
- gradient : 미분을 계산하는 함수  
- init : 시작점  
- lr : 학습률(미분을 통해 업데이트 하는 속도 조절)  
- eps : 알고리즘 종료 조건 (컴퓨터로 계산할 때 미분이 정확히 0이 되는 것은 불가능하므로, eps보다 작을 때 종료하는 조건 필요)  

```python
var = init
grad = gradient(var)
while(abs(grad) > eps): # 절대값
    var var - lr * grad
    grad = gradient(var)
```  

- 함수가 f(x) = x² + 2x + 3일 때 경사하강법으로 최소점 찾는 코드  

```python
import numpy as np

def func(val):
    fun = sym.poly(x**2 + 2*x + 3)
    return fun.subs(x, val), fun

def func_gradient(fun, val):
    _, function = fun(val)
    diff = sym.diff(function, x)
    return diff.subs(x, val), diff

def gradient_descent(fun, init_point, lr_rate=2, epsilon=5):
    cnt=0
    val = init_point
    diff, _ = func_gradient(fun, init_point)
    while np.abs(diff) > epsilon:
        val = val - lr_rate*diff
        diff, _ = func_gradient(fun, val)
        cnt+=1
    print("함수: {}, 연산횟수: {}, 최소점: ({}, {})".format(fun(val)[1], cnt, val, fun(val)[0]))

gradient_descent(fun=func, init_point=np.random.uniform(-2,2))
```  
>  
결과  
```
    함수: Poly(x**2 + 2*x + 3, x, domain='ZZ'), 연산횟수: 0, 최소점: (-0.6218640860666538, 2.14298676940621)
```  

### 변수가 벡터이면?  
- 미분(differentiation)은 변수의 움직임에 따른 함수값의 변화를 측정하기 위한 도구로 최적화에서 제일 많이 사용하는 기법  
- 벡터가 입력인 다변수 함수의 경우 편미분(partial differentiation)을 사용한다  
- 각 변수 별로 편미분을 계산한 그레디언트(gradient)벡터를 이용하여 경사하강/경사상승법을 사용할 수 있다  

![AI_class_01_01_14_07](/images/2022-01-29-AI_class_01_01_14/AI_class_01_01_14_07.png){: width="50%" height="50%"}{: .center}  

-- y는 상관 없고, x방향의 움직임만 미분했다  

```python
import sympy as sym
from sympy.abc import x, y

sym.diff(sym.poly(x**2 + 2*x*y + 3) + sym.cos(x + 2*y), x)
```  
>  
결과  
```
    2𝑥+2𝑦−sin(𝑥+2𝑦)
```  

- ▽ : nabla (나블라)

![AI_class_01_01_14_08](/images/2022-01-29-AI_class_01_01_14/AI_class_01_01_14_08.png){: width="50%" height="50%"}{: .center}  

### 그레디언트 벡터  
- (x, y, z) 3차원 공간에서 (f(x),y)표면을 따라서 그레디언트 벡터에 마이너스를 붙이면 (f(x),y)의 극소점으로 향하는 화살표들의 움직임을 볼 수 있다  
- (f(x), y)의 어떤점에서 출발하든지간에 마이너스 그레디언트 f 방향으로 따라가기만 하면 (f(x), y)의 최소점으로 흐르게 된다  

![AI_class_01_01_14_09](/images/2022-01-29-AI_class_01_01_14/AI_class_01_01_14_09.png){: width="50%" height="50%"}{: .center}  

- 그레디언트 벡터 ▽f(x,y)는 원점에서 가장 빨리 증가하는 방향으로 흐르게 된다  

![AI_class_01_01_14_10](/images/2022-01-29-AI_class_01_01_14/AI_class_01_01_14_10.png){: width="50%" height="50%"}{: .center}  

- 마이너스 그레디언트 벡터 -▽f는 ▽(-f)랑 같고, 임의의 점에서 출발해서 최소점이 0,0으로 가장 빨리 감소하게 되는 방향으로 움직인다  

![AI_class_01_01_14_11](/images/2022-01-29-AI_class_01_01_14/AI_class_01_01_14_11.png){: width="50%" height="50%"}{: .center}  

## 경사하강법 알고리즘  
- Input : gradient, init, lr, eps / Output : var  
- gradient : 그레디언트 벡터를 계산하는 함수  
- init : 시작점  
- lr : 학습률(미분을 통해 업데이트 하는 속도 조절)  
- eps : 알고리즘 종료 조건 (노름(norm)을 계산해서 종료 조건 설정)  

```python
var = init
grad = gradient(var)
while(norm(grad) > eps):
    var var - lr * grad
    grad = gradient(var)
```  

```python
import numpy as np

def eval_(fun, val):
    val_x, val_y = val
    fun_eval = fun.subs(x, val_x).subs(y, val_y)
    return fun_eval

def func_multi(val):
    x_,y_ = val
    func = sym.poly(x**2 +2*y**2)
    return eval_(func, [x_,y_]), func

def func_gradient(fun,val):
    x_,y_ = val
    _, function = fun(val)
    diff_x = sym.diff(function, x)
    diff_y = sym.diff(function, y)
    grad_vec = np.array([eval_(diff_x, [x_,y_]), eval_(diff_y, [x_, y_])], dtype=float)
    return grad_vec, [diff_x, diff_y]

def gradient_descent(fun, init_point, lr_rate=2, epsilon=5):
    cnt = 0
    val = init_point
    diff, _ = func_gradient(fun,val)
    while np.linalg.norm(diff) > epsilon:
        val = val - lr_rate*diff
        diff, _ = func_gradient(fun, val)
        cnt+=1
    print("함수: {}, 연산횟수: {}, 최소점: ({}, {})".format(fun(val)[1], cnt, val, fun(val)[0]))

pt = [np.random.uniform(-2,2), np.random.uniform(-2,2)]
gradient_descent(fun = func_multi, init_point=pt)
```  
>  
결과  
```
    함수: Poly(x**2 + 2*y**2, x, y, domain='ZZ'), 연산횟수: 0, 최소점: ([-0.04111846336645808, 0.8252983506199203], 1.36392546310154)
```  


임성빈 교수의 인공지능(AI) 강의를 들으며 정리하였습니다.  
또한 이곳저곳에서 구글링하며 공부하였습니다.  